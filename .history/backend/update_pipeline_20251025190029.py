# # update_pipeline.py
# import pandas as pd
# import sqlite3
# from datetime import datetime
# import os

# # Import your processing functions
# from models.preprocess import organize_records, clean_and_fill

# LATEST_CSV_PATH = "data/latest.csv" # Path to the file from GitHub Actions
# DB_PATH = "database/water_quality.db"
# TABLE_NAME = "water_records"

# def fetch_and_update_data():
#     """
#     Fetches the latest.csv, processes it, and appends to the database.
#     """
#     print(f"[{datetime.now()}] Running hourly data fetch...")
    
#     # Check if the file exists
#     if not os.path.exists(LATEST_CSV_PATH):
#         print(f"Hourly fetch: 'data/latest.csv' not found. Skipping update.")
#         return
        
#     try:
#         # 1. Read latest.csv
#         raw_df = pd.read_csv(LATEST_CSV_PATH)
        
#         # 2. Process it (Step 3: Organize)
#         organized_df = organize_records(raw_df)
        
#         # 3. Process it (Step 4: Clean)
#         clean_df = clean_and_fill(organized_df)
        
#         if clean_df.empty:
#             print("Hourly fetch: No new valid data to add.")
#             return
            
#         # 4. Append to database
#         con = sqlite3.connect(DB_PATH)
#         clean_df.to_sql(TABLE_NAME, con, if_exists='append', index=False)
#         con.close()
        
#         print(f"âœ… Success: Added {len(clean_df)} new rows to the database.")
        
#     except Exception as e:
#         print(f"ðŸ”´ ERROR during hourly update: {e}")



# backend/update_pipeline.py
import pandas as pd
import numpy as np # Import numpy for NaN handling
import sqlite3
import json
import os
from datetime import datetime

# Assuming 'clean_and_fill' handles numeric conversion and NaN filling
# If it doesn't exist or do that, we'll need to add that logic here.
# For now, let's assume it exists and works on a DataFrame.
try:
    from models.preprocess import clean_and_fill
    PREPROCESS_AVAILABLE = True
except ImportError:
    print("Warning: 'models.preprocess.clean_and_fill' not found. Basic cleaning will be applied.")
    PREPROCESS_AVAILABLE = False
    # Define a placeholder function if needed, or add logic directly below
    def clean_and_fill(df):
        print("Applying basic cleaning (numeric conversion, timestamp parsing)...")
         # Ensure timestamp column exists and convert to datetime objects
        if 'timestamp' not in df.columns:
            print("ðŸ”´ ERROR: 'timestamp' column missing.")
            return pd.DataFrame() # Return empty if essential column missing
        try:
            # Assuming timestamp is like 'YYYY-MM-DD HH:MM:SS' from scraper
            df['timestampDate'] = pd.to_datetime(df['timestamp'], errors='coerce')
        except Exception as e:
            print(f"ðŸ”´ ERROR converting 'timestamp' column: {e}")
            df['timestampDate'] = pd.NaT

        df = df.dropna(subset=['stationId', 'timestampDate'])
        if df.empty: return df

        # Convert potential parameter columns to numeric
        meta_cols = ['stationId', 'stationName', 'location', 'timestamp', 'timestampDate', 'id']
        param_cols = [col for col in df.columns if col not in meta_cols]
        for col in param_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        # Simple fillna example (replace with more sophisticated logic if needed)
        # df[param_cols] = df[param_cols].fillna(0)
        return df


# --- Configuration ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_DIR = SCRIPT_DIR

DB_PATH = os.path.join(BACKEND_DIR, "database/water_quality.db")
DB_DIR = os.path.dirname(DB_PATH)
os.makedirs(DB_DIR, exist_ok=True)
TABLE_NAME = "water_records"
# --- Use the JSON file generated by the scraper ---
SCRAPED_DATA_JSON = os.path.join(BACKEND_DIR, "static/scraped_data/latest_cpcb_data.json")

def preprocess_and_store_data():
    """
    Reads the latest scraped JSON data, processes it using clean_and_fill,
    and stores it in the SQLite DB.
    """
    print(f"[{datetime.now()}] --- Starting Database Update Pipeline ---")

    # --- 1. Read Scraped JSON Data ---
    print(f"Reading scraped data from: {SCRAPED_DATA_JSON}")
    if not os.path.exists(SCRAPED_DATA_JSON):
        print(f"ðŸ”´ ERROR: Scraped data file not found at {SCRAPED_DATA_JSON}. Cannot update database.")
        return

    try:
        # Read directly into pandas DataFrame
        df = pd.read_json(SCRAPED_DATA_JSON, orient='records')
        if df.empty:
            print("ðŸŸ¡ Scraped data file is empty. No data to update.")
            return
        print(f"Loaded {len(df)} records from JSON.")
    except ValueError as e: # Catch JSON parsing errors specifically
        print(f"ðŸ”´ ERROR: Failed to parse JSON file '{SCRAPED_DATA_JSON}': {e}")
        return
    except Exception as e:
        print(f"ðŸ”´ ERROR: Failed to read JSON file: {e}")
        return

    # --- 2. Clean Data ---
    print("Cleaning and preparing data...")
    # We assume clean_and_fill handles timestamp parsing, numeric conversion, NaNs
    # It should add the 'timestampDate' column based on 'timestamp'
    processed_df = clean_and_fill(df.copy()) # Pass a copy

    if processed_df.empty:
        print("ðŸŸ¡ No valid data remaining after cleaning. Skipping database update.")
        return

    # --- 3. Prepare for Database Insertion ---
    # Ensure stationId is present
    if 'stationId' not in processed_df.columns:
        print("ðŸ”´ ERROR: 'stationId' column missing after cleaning.")
        return
    # Ensure timestampDate is present and valid
    if 'timestampDate' not in processed_df.columns or processed_df['timestampDate'].isnull().all():
         print("ðŸ”´ ERROR: 'timestampDate' column missing or invalid after cleaning.")
         return

    # Identify parameter columns (all except known metadata)
    meta_cols = ['stationId', 'stationName', 'location', 'timestamp', 'timestampDate', 'id']
    param_cols = sorted([col for col in processed_df.columns if col not in meta_cols]) # Sort for consistent table creation
    db_cols = ['stationId', 'timestamp', 'timestampDate'] + param_cols
    df_to_store = processed_df[[col for col in db_cols if col in processed_df.columns]].copy()
    # Replace any remaining Pandas NA types with None for SQLite compatibility
    df_to_store = df_to_store.replace({pd.NA: None, np.nan: None})

    # Convert datetime objects to ISO 8601 strings using strftime
    df_to_store['timestampDate'] = df_to_store['timestampDate'].dt.strftime('%Y-%m-%dT%H:%M:%S')

    # --- 4. Store Data in SQLite Database ---
    print(f"Connecting to database: {DB_PATH}")
    inserted_count = 0
    try:
        with sqlite3.connect(DB_PATH) as con:
            cur = con.cursor()
            # Define column types for CREATE TABLE statement
            cols_sql_parts = []
            for col in param_cols:
                 # Check dtype after cleaning - use REAL for floats, INTEGER for ints, TEXT otherwise
                 if pd.api.types.is_float_dtype(df_to_store[col]):
                     col_type = "REAL"
                 elif pd.api.types.is_integer_dtype(df_to_store[col]):
                     col_type = "INTEGER"
                 else:
                     col_type = "TEXT" # Fallback
                 cols_sql_parts.append(f'"{col}" {col_type}') # Quote column names
            cols_sql = ", ".join(cols_sql_parts)

            # Create table if it doesn't exist
            create_table_sql = f"""
            CREATE TABLE IF NOT EXISTS "{TABLE_NAME}" (
                "stationId" TEXT,
                "timestampDate" TIMESTAMP,
                {cols_sql},
                PRIMARY KEY ("stationId", "timestampDate")
            );
            """
            cur.execute(create_table_sql)

            # Insert or Replace using loop for better control and error handling
            print(f"Inserting/updating {len(df_to_store)} records into '{TABLE_NAME}'...")
            cols_placeholders = ', '.join(['?'] * len(df_to_store.columns))
            quoted_cols = ', '.join([f'"{col}"' for col in df_to_store.columns])
            sql = f'INSERT OR REPLACE INTO "{TABLE_NAME}" ({quoted_cols}) VALUES ({cols_placeholders})'

            # Convert DataFrame rows to tuples for executemany
            data_tuples = [tuple(x) for x in df_to_store.to_numpy()]

            cur.executemany(sql, data_tuples)
            con.commit() # Commit changes
            inserted_count = cur.rowcount if cur.rowcount >= 0 else len(df_to_store) # executemany might return -1

            print(f"âœ… Success: {inserted_count} records inserted/replaced in the database.")

    except sqlite3.Error as e:
        print(f"ðŸ”´ ERROR: Database error during insert/replace: {e}")
    except Exception as e:
        print(f"ðŸ”´ ERROR: Failed to store data in database: {e}")

    print("--- Database Update Pipeline Finished ---")

# Allow running this script directly
if __name__ == "__main__":
    preprocess_and_store_data()