name: Hourly Scrape and Daily Batch Jobs

on:
  workflow_dispatch: # Allows manual triggering
  schedule:
    # Runs near the beginning of every hour
    - cron: '5 * * * *' # Run 5 mins past the hour

jobs:
  # Job 1: Scrape data and commit the result JSON
  scrape-and-commit:
    runs-on: ubuntu-latest
    outputs:
      data_updated: ${{ steps.commit_data.outputs.committed }} # Output whether commit happened
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install Scraper Dependencies
        run: |
          python -m pip install --upgrade pip
          # Ensure requests and pandas are installed (json/os are built-in)
          pip install pandas requests numpy # Added numpy as scraper uses it

      - name: Create output directory
        run: mkdir -p backend/static/scraped_data

      - name: Run CPCB Scraper
        # Assumes cpcb_scraper.py is in the root, adjust path if needed
        run: python cpcb_scraper.py

      - name: Commit Scraped Data File
        id: commit_data # Give the step an ID to reference its output
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add backend/static/scraped_data/latest_cpcb_data.json
          # Commit only if the tracked file changed, and set output
          if git diff --staged --quiet; then
            echo "No changes to scraped data."
            echo "committed=false" >> $GITHUB_OUTPUT
          else
            git commit -m "Update CPCB water quality data"
            echo "committed=true" >> $GITHUB_OUTPUT
          fi
          # Always attempt push, even if no commit (in case of force push/rebase scenarios)
          git push

  # Job 2: Update database and run batch jobs (daily)
  update-db-and-run-jobs:
    runs-on: ubuntu-latest
    needs: scrape-and-commit # Run only after scrape job finishes
    # Optional: Run only if scraper committed new data?
    # if: needs.scrape-and-commit.outputs.data_updated == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # Fetch full history needed if batch jobs rely on past data trends
        with:
          fetch-depth: 0 # Fetch all history for all tags and branches

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install Full Dependencies (for pipeline and batch jobs)
        run: |
          python -m pip install --upgrade pip
          # Install everything needed by update_pipeline AND run_all_batch_jobs
          # Adjust this list based on your actual requirements.txt
          pip install pandas requests numpy tensorflow scikit-learn plotly joblib apscheduler Flask Flask-Cors

      - name: Run Database Update Pipeline
        # Assumes update_pipeline.py is in backend/
        run: python backend/update_pipeline.py

      # --- Daily Batch Job Execution ---
      # Runs only once a day, e.g., around midnight UTC (adjust cron as needed)
      # Uses github.run_attempt to avoid re-running on retries within the same hour
      - name: Run All Batch Jobs (Daily at ~00:05 UTC)
        if: github.run_attempt == '1' && startsWith(github.event.schedule, '5 0 * * *') # Check cron schedule
        run: |
          echo "Running daily batch jobs..."
          # Assumes run_all_batch_jobs.py is in the root
          python run_all_batch_jobs.py

      - name: Commit Generated Plot Files (if daily jobs ran)
        # Only run this if the daily batch jobs step ran
        if: github.run_attempt == '1' && startsWith(github.event.schedule, '5 0 * * *')
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          # Add all potentially generated/updated files (plots, summaries)
          git add backend/static/predictions/daily/*.json
          git add backend/static/predictions/weekly/*.json
          git add backend/static/predictions/weekly_details/*.json
          git add backend/static/predictions/*.json # Summary files
          git add backend/static/correlation/*.json
          git add backend/static/anomaly/*.json
          git add backend/static/daynight/*.png
          # Commit only if files changed
          if git diff --staged --quiet; then
            echo "No changes to generated plots/summaries."
          else
            git commit -m "Update generated analysis plots and summaries"
            git push
          fi